
==> Audit <==
|---------|----------------------|----------|-------------------------------|---------|---------------------|---------------------|
| Command |         Args         | Profile  |             User              | Version |     Start Time      |      End Time       |
|---------|----------------------|----------|-------------------------------|---------|---------------------|---------------------|
| start   |                      | minikube | 8B06633D60235A6\Administrator | v1.35.0 | 09 Apr 25 11:56 IST | 09 Apr 25 12:01 IST |
| service | book-scraper-service | minikube | 8B06633D60235A6\Administrator | v1.35.0 | 09 Apr 25 12:02 IST |                     |
|---------|----------------------|----------|-------------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/04/09 11:56:27
Running on machine: 8b06633d60235a6
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0409 11:56:27.271015   13760 out.go:345] Setting OutFile to fd 92 ...
I0409 11:56:27.273015   13760 out.go:397] isatty.IsTerminal(92) = true
I0409 11:56:27.273015   13760 out.go:358] Setting ErrFile to fd 96...
I0409 11:56:27.273015   13760 out.go:397] isatty.IsTerminal(96) = true
W0409 11:56:27.300380   13760 root.go:314] Error reading config file at C:\Users\Administrator\.minikube\config\config.json: open C:\Users\Administrator\.minikube\config\config.json: The system cannot find the path specified.
I0409 11:56:27.320617   13760 out.go:352] Setting JSON to false
I0409 11:56:27.330616   13760 start.go:129] hostinfo: {"hostname":"8b06633d60235a6","uptime":7437,"bootTime":1744172549,"procs":246,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19044.2130 Build 19044.2130","kernelVersion":"10.0.19044.2130 Build 19044.2130","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"eaad01dd-4ce6-45e0-91b4-c992a1351d0a"}
W0409 11:56:27.330616   13760 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0409 11:56:27.340619   13760 out.go:177] ðŸ˜„  minikube v1.35.0 on Microsoft Windows 10 Pro 10.0.19044.2130 Build 19044.2130
W0409 11:56:27.346616   13760 preload.go:293] Failed to list preload files: open C:\Users\Administrator\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I0409 11:56:27.351619   13760 notify.go:220] Checking for updates...
I0409 11:56:27.356620   13760 driver.go:394] Setting default libvirt URI to qemu:///system
I0409 11:56:27.357616   13760 global.go:112] Querying for installed drivers using PATH=D:\Oracle\bin;C:\Program Files\Python310\Scripts\;C:\Program Files\Python310\;C:\Program Files\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Program Files\Java\jdk-11.0.16.1\bin;C:\Program Files\Amazon\AWSCLIV2\;C:\Program Files (x86)\Micro Focus\LoadRunner\bin\Silk\Silk Performer 21.0;C:\Program Files\nodejs\;C:\Program Files\nodejs\node_modules\npm\bin;C:\apache-jmeter-5.5\bin;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\PuTTY\;C:\ProgramData\chocolatey\bin;C:\Program Files\Git\cmd;C:\Minikube;C:\Program Files\MySQL\MySQL Shell 8.0\bin\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;C:\Users\Administrator\AppData\Local\Programs\Microsoft VS Code\bin;C:\Program Files\JetBrains\IntelliJ IDEA Community Edition 2022.3.2\bin;;C:\Users\Administrator\AppData\Roaming\npm
I0409 11:56:28.950211   13760 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0409 11:56:28.973774   13760 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0409 11:56:29.019175   13760 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0409 11:56:29.033704   13760 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0409 11:56:29.289127   13760 docker.go:123] docker version: linux-20.10.22:Docker Desktop 4.16.3 (96739)
I0409 11:56:29.309121   13760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0409 11:56:29.963089   13760 info.go:266] docker info: {ID:K57M:SGTP:KHAN:XVWW:QALY:KBCH:PUWG:BVLI:AGSX:6U6V:2JXO:7AQP Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:47 OomKillDisable:false NGoroutines:53 SystemTime:2025-04-09 06:26:29.4516507 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:6 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:2081345536 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9ba4b250366a5ddde94bb7c9d1def331423aa323 Expected:9ba4b250366a5ddde94bb7c9d1def331423aa323} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.15.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.17] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0409 11:56:29.963089   13760 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0409 11:56:29.982741   13760 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0409 11:56:29.982815   13760 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0409 11:56:29.983450   13760 driver.go:316] not recommending "ssh" due to default: false
I0409 11:56:29.983450   13760 driver.go:351] Picked: docker
I0409 11:56:29.984036   13760 driver.go:352] Alternatives: [hyperv ssh]
I0409 11:56:29.984036   13760 driver.go:353] Rejects: [qemu2 virtualbox vmware podman]
I0409 11:56:29.988117   13760 out.go:177] âœ¨  Automatically selected the docker driver. Other choices: hyperv, ssh
I0409 11:56:29.991200   13760 start.go:297] selected driver: docker
I0409 11:56:29.991200   13760 start.go:901] validating driver "docker" against <nil>
I0409 11:56:29.991726   13760 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0409 11:56:30.027111   13760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0409 11:56:30.634659   13760 info.go:266] docker info: {ID:K57M:SGTP:KHAN:XVWW:QALY:KBCH:PUWG:BVLI:AGSX:6U6V:2JXO:7AQP Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:47 OomKillDisable:false NGoroutines:53 SystemTime:2025-04-09 06:26:30.1550884 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:6 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:2081345536 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9ba4b250366a5ddde94bb7c9d1def331423aa323 Expected:9ba4b250366a5ddde94bb7c9d1def331423aa323} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.15.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.17] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0409 11:56:30.636690   13760 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0409 11:56:30.695990   13760 start_flags.go:393] Using suggested 1984MB memory alloc based on sys=16382MB, container=1984MB
I0409 11:56:30.696629   13760 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0409 11:56:30.698804   13760 out.go:177] ðŸ“Œ  Using Docker Desktop driver with root privileges
I0409 11:56:30.710795   13760 cni.go:84] Creating CNI manager for ""
I0409 11:56:30.711796   13760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0409 11:56:30.711796   13760 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0409 11:56:30.713801   13760 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1984 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0409 11:56:30.716796   13760 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0409 11:56:30.722393   13760 cache.go:121] Beginning downloading kic base image for docker with docker
I0409 11:56:30.725807   13760 out.go:177] ðŸšœ  Pulling base image v0.0.46 ...
I0409 11:56:30.727793   13760 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0409 11:56:30.728798   13760 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0409 11:56:30.925401   13760 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0409 11:56:30.927398   13760 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0409 11:56:30.927398   13760 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0409 11:56:30.928396   13760 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0409 11:56:30.929407   13760 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0409 11:56:30.981929   13760 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0409 11:56:30.982929   13760 cache.go:56] Caching tarball of preloaded images
I0409 11:56:30.982929   13760 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0409 11:56:30.986927   13760 out.go:177] ðŸ’¾  Downloading Kubernetes v1.32.0 preload ...
I0409 11:56:30.988933   13760 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0409 11:56:31.332470   13760 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0409 11:56:43.603288   13760 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0409 11:56:43.603288   13760 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0409 11:56:43.604107   13760 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0409 11:57:43.866507   13760 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0409 11:59:21.616418   13760 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0409 11:59:21.622416   13760 preload.go:254] verifying checksum of C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0409 11:59:23.419084   13760 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0409 11:59:23.424870   13760 profile.go:143] Saving config to C:\Users\Administrator\.minikube\profiles\minikube\config.json ...
I0409 11:59:23.425862   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\config.json: {Name:mk72497029ba331d03cccfebd8d8cfdf8706a2c1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 11:59:23.432876   13760 cache.go:227] Successfully downloaded all kic artifacts
I0409 11:59:23.455507   13760 start.go:360] acquireMachinesLock for minikube: {Name:mk3f259f80712de9f83f91ad0f12658ee47ef5bf Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0409 11:59:23.455507   13760 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0409 11:59:23.457825   13760 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1984 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0409 11:59:23.458455   13760 start.go:125] createHost starting for "" (driver="docker")
I0409 11:59:23.472422   13760 out.go:235] ðŸ”¥  Creating docker container (CPUs=2, Memory=1984MB) ...
I0409 11:59:23.574434   13760 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0409 11:59:23.575494   13760 client.go:168] LocalClient.Create starting
I0409 11:59:23.586462   13760 main.go:141] libmachine: Creating CA: C:\Users\Administrator\.minikube\certs\ca.pem
I0409 11:59:24.419335   13760 main.go:141] libmachine: Creating client certificate: C:\Users\Administrator\.minikube\certs\cert.pem
I0409 11:59:25.131105   13760 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0409 11:59:25.781557   13760 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0409 11:59:25.797789   13760 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0409 11:59:25.797789   13760 cli_runner.go:164] Run: docker network inspect minikube
W0409 11:59:26.156897   13760 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0409 11:59:26.156897   13760 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I0409 11:59:26.156897   13760 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I0409 11:59:26.169170   13760 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0409 11:59:26.503422   13760 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0013813b0}
I0409 11:59:26.504421   13760 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0409 11:59:26.516423   13760 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0409 11:59:27.399555   13760 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0409 11:59:27.400546   13760 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0409 11:59:27.426547   13760 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0409 11:59:27.825025   13760 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0409 11:59:28.268824   13760 oci.go:103] Successfully created a docker volume minikube
I0409 11:59:28.278938   13760 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0409 11:59:33.086296   13760 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (4.8073585s)
I0409 11:59:33.086296   13760 oci.go:107] Successfully prepared a docker volume minikube
I0409 11:59:33.086296   13760 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0409 11:59:33.086296   13760 kic.go:194] Starting extracting preloaded images to volume ...
I0409 11:59:33.099719   13760 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
W0409 11:59:43.198777   13760 cli_runner.go:211] docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir returned with exit code 125
I0409 11:59:43.198777   13760 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (10.0990588s)
I0409 11:59:43.199655   13760 kic.go:201] Unable to extract preloaded tarball to volume: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: exit status 125
stdout:

stderr:
docker: Error response from daemon: user declined directory sharing C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4.
See 'docker run --help'.
I0409 11:59:43.213743   13760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0409 11:59:44.256819   13760 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.0422058s)
I0409 11:59:44.256819   13760 info.go:266] docker info: {ID:K57M:SGTP:KHAN:XVWW:QALY:KBCH:PUWG:BVLI:AGSX:6U6V:2JXO:7AQP Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:49 OomKillDisable:false NGoroutines:56 SystemTime:2025-04-09 06:29:43.4253423 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:6 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:2081345536 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.22 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9ba4b250366a5ddde94bb7c9d1def331423aa323 Expected:9ba4b250366a5ddde94bb7c9d1def331423aa323} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.15.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.5] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.17] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.23.0]] Warnings:<nil>}}
I0409 11:59:44.278690   13760 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0409 11:59:45.199674   13760 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=1984mb --memory-swap=1984mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0409 11:59:47.570115   13760 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=1984mb --memory-swap=1984mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279: (2.3704413s)
I0409 11:59:47.585290   13760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0409 11:59:47.874192   13760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0409 11:59:48.169727   13760 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0409 11:59:48.641738   13760 oci.go:144] the created container "minikube" has a running status.
I0409 11:59:48.644452   13760 kic.go:225] Creating ssh key for kic: C:\Users\Administrator\.minikube\machines\minikube\id_rsa...
I0409 11:59:48.910218   13760 kic_runner.go:191] docker (temp): C:\Users\Administrator\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0409 11:59:49.561733   13760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0409 11:59:49.886883   13760 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0409 11:59:49.886883   13760 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0409 11:59:50.333014   13760 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Administrator\.minikube\machines\minikube\id_rsa...
I0409 11:59:51.320240   13760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0409 11:59:51.630899   13760 machine.go:93] provisionDockerMachine start ...
I0409 11:59:51.642910   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:51.891984   13760 main.go:141] libmachine: Using SSH client type: native
I0409 11:59:51.909069   13760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1035360] 0x1037ea0 <nil>  [] 0s} 127.0.0.1 6772 <nil> <nil>}
I0409 11:59:51.909069   13760 main.go:141] libmachine: About to run SSH command:
hostname
I0409 11:59:52.415603   13760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0409 11:59:52.417926   13760 ubuntu.go:169] provisioning hostname "minikube"
I0409 11:59:52.432003   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:52.745484   13760 main.go:141] libmachine: Using SSH client type: native
I0409 11:59:52.746486   13760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1035360] 0x1037ea0 <nil>  [] 0s} 127.0.0.1 6772 <nil> <nil>}
I0409 11:59:52.746486   13760 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0409 11:59:53.199124   13760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0409 11:59:53.228497   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:53.460378   13760 main.go:141] libmachine: Using SSH client type: native
I0409 11:59:53.461123   13760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1035360] 0x1037ea0 <nil>  [] 0s} 127.0.0.1 6772 <nil> <nil>}
I0409 11:59:53.461123   13760 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0409 11:59:53.798338   13760 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0409 11:59:53.800118   13760 ubuntu.go:175] set auth options {CertDir:C:\Users\Administrator\.minikube CaCertPath:C:\Users\Administrator\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Administrator\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Administrator\.minikube\machines\server.pem ServerKeyPath:C:\Users\Administrator\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Administrator\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Administrator\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Administrator\.minikube}
I0409 11:59:53.800118   13760 ubuntu.go:177] setting up certificates
I0409 11:59:53.800118   13760 provision.go:84] configureAuth start
I0409 11:59:53.817847   13760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0409 11:59:54.060554   13760 provision.go:143] copyHostCerts
I0409 11:59:54.061490   13760 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\ca.pem --> C:\Users\Administrator\.minikube/ca.pem (1099 bytes)
I0409 11:59:54.063311   13760 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\cert.pem --> C:\Users\Administrator\.minikube/cert.pem (1139 bytes)
I0409 11:59:54.065680   13760 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\key.pem --> C:\Users\Administrator\.minikube/key.pem (1675 bytes)
I0409 11:59:54.067339   13760 provision.go:117] generating server cert: C:\Users\Administrator\.minikube\machines\server.pem ca-key=C:\Users\Administrator\.minikube\certs\ca.pem private-key=C:\Users\Administrator\.minikube\certs\ca-key.pem org=Administrator.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0409 11:59:54.163277   13760 provision.go:177] copyRemoteCerts
I0409 11:59:54.186648   13760 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0409 11:59:54.211314   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:54.485123   13760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:6772 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0409 11:59:54.696305   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I0409 11:59:54.879728   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0409 11:59:55.009395   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0409 11:59:55.141914   13760 provision.go:87] duration metric: took 1.3397994s to configureAuth
I0409 11:59:55.141914   13760 ubuntu.go:193] setting minikube options for container-runtime
I0409 11:59:55.147837   13760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0409 11:59:55.183138   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:55.439497   13760 main.go:141] libmachine: Using SSH client type: native
I0409 11:59:55.439673   13760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1035360] 0x1037ea0 <nil>  [] 0s} 127.0.0.1 6772 <nil> <nil>}
I0409 11:59:55.439673   13760 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0409 11:59:55.817242   13760 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0409 11:59:55.817242   13760 ubuntu.go:71] root file system type: overlay
I0409 11:59:55.817242   13760 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0409 11:59:55.827249   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:56.101507   13760 main.go:141] libmachine: Using SSH client type: native
I0409 11:59:56.102312   13760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1035360] 0x1037ea0 <nil>  [] 0s} 127.0.0.1 6772 <nil> <nil>}
I0409 11:59:56.102312   13760 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0409 11:59:56.481335   13760 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0409 11:59:56.495141   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:56.780362   13760 main.go:141] libmachine: Using SSH client type: native
I0409 11:59:56.780362   13760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1035360] 0x1037ea0 <nil>  [] 0s} 127.0.0.1 6772 <nil> <nil>}
I0409 11:59:56.780362   13760 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0409 11:59:59.454055   13760 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-04-09 06:29:56.478265000 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0409 11:59:59.454055   13760 machine.go:96] duration metric: took 7.8231553s to provisionDockerMachine
I0409 11:59:59.454055   13760 client.go:171] duration metric: took 35.8785609s to LocalClient.Create
I0409 11:59:59.454055   13760 start.go:167] duration metric: took 35.8796207s to libmachine.API.Create "minikube"
I0409 11:59:59.455072   13760 start.go:293] postStartSetup for "minikube" (driver="docker")
I0409 11:59:59.455072   13760 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0409 11:59:59.472061   13760 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0409 11:59:59.485056   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 11:59:59.764201   13760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:6772 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0409 11:59:59.974672   13760 ssh_runner.go:195] Run: cat /etc/os-release
I0409 11:59:59.991081   13760 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0409 11:59:59.991081   13760 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0409 11:59:59.991081   13760 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0409 11:59:59.991081   13760 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0409 11:59:59.999637   13760 filesync.go:126] Scanning C:\Users\Administrator\.minikube\addons for local assets ...
I0409 12:00:00.006717   13760 filesync.go:126] Scanning C:\Users\Administrator\.minikube\files for local assets ...
I0409 12:00:00.007702   13760 start.go:296] duration metric: took 552.6293ms for postStartSetup
I0409 12:00:00.042292   13760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0409 12:00:00.273444   13760 profile.go:143] Saving config to C:\Users\Administrator\.minikube\profiles\minikube\config.json ...
I0409 12:00:00.318783   13760 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0409 12:00:00.342982   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 12:00:00.632546   13760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:6772 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0409 12:00:00.856034   13760 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0409 12:00:00.874713   13760 start.go:128] duration metric: took 37.4154276s to createHost
I0409 12:00:00.875240   13760 start.go:83] releasing machines lock for "minikube", held for 37.4197333s
I0409 12:00:00.889607   13760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0409 12:00:01.119918   13760 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0409 12:00:01.133043   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 12:00:01.133521   13760 ssh_runner.go:195] Run: cat /version.json
I0409 12:00:01.159910   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 12:00:01.382871   13760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:6772 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0409 12:00:01.416641   13760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:6772 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
W0409 12:00:01.536754   13760 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0409 12:00:01.605051   13760 ssh_runner.go:195] Run: systemctl --version
I0409 12:00:01.681548   13760 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0409 12:00:01.715539   13760 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0409 12:00:01.764136   13760 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0409 12:00:01.786113   13760 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0409 12:00:01.895007   13760 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0409 12:00:01.895007   13760 start.go:495] detecting cgroup driver to use...
I0409 12:00:01.895567   13760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0409 12:00:01.898830   13760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0409 12:00:01.967615   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0409 12:00:02.006181   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0409 12:00:02.038054   13760 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0409 12:00:02.066353   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0409 12:00:02.126647   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W0409 12:00:02.133802   13760 out.go:270] â—  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0409 12:00:02.136844   13760 out.go:270] ðŸ’¡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0409 12:00:02.227683   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0409 12:00:02.311123   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0409 12:00:02.362696   13760 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0409 12:00:02.405343   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0409 12:00:02.449528   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0409 12:00:02.495124   13760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0409 12:00:02.544355   13760 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0409 12:00:02.592160   13760 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0409 12:00:02.633134   13760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0409 12:00:02.838396   13760 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0409 12:00:03.055407   13760 start.go:495] detecting cgroup driver to use...
I0409 12:00:03.055407   13760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0409 12:00:03.071390   13760 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0409 12:00:03.108391   13760 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0409 12:00:03.127390   13760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0409 12:00:03.157287   13760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0409 12:00:03.219605   13760 ssh_runner.go:195] Run: which cri-dockerd
I0409 12:00:03.259267   13760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0409 12:00:03.293299   13760 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0409 12:00:03.363168   13760 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0409 12:00:03.560092   13760 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0409 12:00:03.728693   13760 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0409 12:00:03.729700   13760 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0409 12:00:03.798992   13760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0409 12:00:03.973140   13760 ssh_runner.go:195] Run: sudo systemctl restart docker
I0409 12:00:04.669318   13760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0409 12:00:04.713595   13760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0409 12:00:04.762867   13760 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0409 12:00:04.937741   13760 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0409 12:00:05.086215   13760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0409 12:00:05.264055   13760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0409 12:00:05.317341   13760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0409 12:00:05.364946   13760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0409 12:00:05.539650   13760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0409 12:00:06.194437   13760 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0409 12:00:06.214889   13760 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0409 12:00:06.226416   13760 start.go:563] Will wait 60s for crictl version
I0409 12:00:06.241423   13760 ssh_runner.go:195] Run: which crictl
I0409 12:00:06.267420   13760 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0409 12:00:06.661497   13760 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0409 12:00:06.674222   13760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0409 12:00:06.988341   13760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0409 12:00:07.048632   13760 out.go:235] ðŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0409 12:00:07.066302   13760 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0409 12:00:07.565432   13760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0409 12:00:07.599376   13760 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0409 12:00:07.622077   13760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0409 12:00:07.669053   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0409 12:00:07.919478   13760 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1984 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0409 12:00:07.919478   13760 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0409 12:00:07.930368   13760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0409 12:00:08.000864   13760 docker.go:689] Got preloaded images: 
I0409 12:00:08.000864   13760 docker.go:695] registry.k8s.io/kube-apiserver:v1.32.0 wasn't preloaded
I0409 12:00:08.037347   13760 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0409 12:00:08.104093   13760 ssh_runner.go:195] Run: which lz4
I0409 12:00:08.148838   13760 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I0409 12:00:08.166008   13760 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0409 12:00:08.166008   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (349777613 bytes)
I0409 12:00:36.671132   13760 docker.go:653] duration metric: took 28.5553108s to copy over tarball
I0409 12:00:36.707961   13760 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I0409 12:00:42.779885   13760 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (6.0719239s)
I0409 12:00:42.779885   13760 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0409 12:00:42.928474   13760 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0409 12:00:42.958008   13760 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I0409 12:00:43.057395   13760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0409 12:00:43.255253   13760 ssh_runner.go:195] Run: sudo systemctl restart docker
I0409 12:00:44.851213   13760 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.5959602s)
I0409 12:00:44.874162   13760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0409 12:00:44.973904   13760 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0409 12:00:44.975014   13760 cache_images.go:84] Images are preloaded, skipping loading
I0409 12:00:44.975056   13760 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0409 12:00:44.978286   13760 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0409 12:00:44.997586   13760 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0409 12:00:45.673364   13760 cni.go:84] Creating CNI manager for ""
I0409 12:00:45.673364   13760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0409 12:00:45.673812   13760 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0409 12:00:45.674109   13760 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0409 12:00:45.675518   13760 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0409 12:00:45.714174   13760 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0409 12:00:45.770587   13760 binaries.go:44] Found k8s binaries, skipping transfer
I0409 12:00:45.788311   13760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0409 12:00:45.825009   13760 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0409 12:00:45.883533   13760 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0409 12:00:45.942711   13760 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0409 12:00:46.025406   13760 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0409 12:00:46.049043   13760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0409 12:00:46.115988   13760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0409 12:00:46.354356   13760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0409 12:00:46.427119   13760 certs.go:68] Setting up C:\Users\Administrator\.minikube\profiles\minikube for IP: 192.168.49.2
I0409 12:00:46.427119   13760 certs.go:194] generating shared ca certs ...
I0409 12:00:46.428678   13760 certs.go:226] acquiring lock for ca certs: {Name:mk38b1aba3ca636999c633fc59ac7bca24a1dea0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:46.428678   13760 certs.go:240] generating "minikubeCA" ca cert: C:\Users\Administrator\.minikube\ca.key
I0409 12:00:46.782258   13760 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\ca.crt ...
I0409 12:00:46.782258   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\ca.crt: {Name:mk2e3cc418c780b7c0b8937bfb2e70e951acb27e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:46.784254   13760 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\ca.key ...
I0409 12:00:46.784254   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\ca.key: {Name:mke2a599cc6bd56b768083deb54c7c5d118494d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:46.785254   13760 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\Administrator\.minikube\proxy-client-ca.key
I0409 12:00:47.156419   13760 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\proxy-client-ca.crt ...
I0409 12:00:47.156419   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\proxy-client-ca.crt: {Name:mk958d5f30f1a89e7deabd7f244a44bfdb7c0750 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.157426   13760 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\proxy-client-ca.key ...
I0409 12:00:47.157426   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\proxy-client-ca.key: {Name:mk5d86d1c8dd81fe0455dd3336dc1460ba9bb723 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.158426   13760 certs.go:256] generating profile certs ...
I0409 12:00:47.159424   13760 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Administrator\.minikube\profiles\minikube\client.key
I0409 12:00:47.160422   13760 crypto.go:68] Generating cert C:\Users\Administrator\.minikube\profiles\minikube\client.crt with IP's: []
I0409 12:00:47.288960   13760 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\profiles\minikube\client.crt ...
I0409 12:00:47.288960   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\client.crt: {Name:mkcc9b0ab8e6e5009e256262a102e33f6b4c5b0c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.290972   13760 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\profiles\minikube\client.key ...
I0409 12:00:47.290972   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\client.key: {Name:mk30c85153bb8b579f384eb94bb7d3e1a95d37c1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.292948   13760 certs.go:363] generating signed profile cert for "minikube": C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0409 12:00:47.292948   13760 crypto.go:68] Generating cert C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0409 12:00:47.645529   13760 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0409 12:00:47.645529   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk42e86e594c0431370d9a4fdf2a5a41f74706b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.647534   13760 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0409 12:00:47.647534   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mke7b9551efbfa0955358538d27b6637a4c3a01f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.648525   13760 certs.go:381] copying C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt
I0409 12:00:47.665531   13760 certs.go:385] copying C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key
I0409 12:00:47.666532   13760 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key
I0409 12:00:47.666532   13760 crypto.go:68] Generating cert C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0409 12:00:47.893087   13760 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt ...
I0409 12:00:47.893087   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt: {Name:mk0e7578614ca016556a64bcd6d1ae8f43f6c356 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.894100   13760 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key ...
I0409 12:00:47.894100   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key: {Name:mk2b6069b395c3db8e1dc812226871b6c6a0d814 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:00:47.909086   13760 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\ca-key.pem (1675 bytes)
I0409 12:00:47.910078   13760 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\ca.pem (1099 bytes)
I0409 12:00:47.910078   13760 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\cert.pem (1139 bytes)
I0409 12:00:47.910078   13760 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\key.pem (1675 bytes)
I0409 12:00:47.930078   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0409 12:00:47.980337   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0409 12:00:48.053515   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0409 12:00:48.160816   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0409 12:00:48.217853   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0409 12:00:48.293054   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0409 12:00:48.381820   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0409 12:00:48.431326   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0409 12:00:48.494527   13760 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0409 12:00:48.554933   13760 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0409 12:00:48.628014   13760 ssh_runner.go:195] Run: openssl version
I0409 12:00:48.709998   13760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0409 12:00:48.806359   13760 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0409 12:00:48.826910   13760 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr  9 06:30 /usr/share/ca-certificates/minikubeCA.pem
I0409 12:00:48.848059   13760 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0409 12:00:48.904028   13760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0409 12:00:48.969503   13760 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0409 12:00:49.002779   13760 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0409 12:00:49.002779   13760 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1984 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0409 12:00:49.022769   13760 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0409 12:00:49.127936   13760 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0409 12:00:49.206052   13760 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0409 12:00:49.236107   13760 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0409 12:00:49.254106   13760 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0409 12:00:49.286917   13760 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0409 12:00:49.286917   13760 kubeadm.go:157] found existing configuration files:

I0409 12:00:49.311637   13760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0409 12:00:49.335624   13760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0409 12:00:49.366779   13760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0409 12:00:49.424040   13760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0409 12:00:49.454578   13760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0409 12:00:49.482581   13760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0409 12:00:49.523457   13760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0409 12:00:49.543457   13760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0409 12:00:49.582624   13760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0409 12:00:49.642759   13760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0409 12:00:49.668684   13760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0409 12:00:49.699388   13760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0409 12:00:49.725245   13760 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0409 12:00:49.931780   13760 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0409 12:00:50.082275   13760 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0409 12:01:08.870017   13760 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0409 12:01:08.871010   13760 kubeadm.go:310] [preflight] Running pre-flight checks
I0409 12:01:08.871830   13760 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0409 12:01:08.873384   13760 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0409 12:01:08.875388   13760 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0409 12:01:08.875388   13760 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0409 12:01:08.883697   13760 out.go:235]     â–ª Generating certificates and keys ...
I0409 12:01:08.885307   13760 kubeadm.go:310] [certs] Using existing ca certificate authority
I0409 12:01:08.885307   13760 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0409 12:01:08.885869   13760 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0409 12:01:08.885869   13760 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0409 12:01:08.885869   13760 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0409 12:01:08.885869   13760 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0409 12:01:08.885869   13760 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0409 12:01:08.885869   13760 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0409 12:01:08.885869   13760 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0409 12:01:08.886398   13760 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0409 12:01:08.886448   13760 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0409 12:01:08.886495   13760 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0409 12:01:08.886495   13760 kubeadm.go:310] [certs] Generating "sa" key and public key
I0409 12:01:08.886495   13760 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0409 12:01:08.886495   13760 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0409 12:01:08.886495   13760 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0409 12:01:08.886495   13760 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0409 12:01:08.887018   13760 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0409 12:01:08.887089   13760 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0409 12:01:08.887089   13760 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0409 12:01:08.887089   13760 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0409 12:01:08.888389   13760 out.go:235]     â–ª Booting up control plane ...
I0409 12:01:08.889122   13760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0409 12:01:08.889416   13760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0409 12:01:08.889416   13760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0409 12:01:08.889416   13760 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0409 12:01:08.889416   13760 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0409 12:01:08.889946   13760 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0409 12:01:08.890164   13760 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0409 12:01:08.890164   13760 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0409 12:01:08.890164   13760 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.502613s
I0409 12:01:08.890164   13760 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0409 12:01:08.890164   13760 kubeadm.go:310] [api-check] The API server is healthy after 10.0028841s
I0409 12:01:08.890983   13760 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0409 12:01:08.890983   13760 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0409 12:01:08.890983   13760 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0409 12:01:08.890983   13760 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0409 12:01:08.890983   13760 kubeadm.go:310] [bootstrap-token] Using token: qhcmb5.xrpe2hv5fpiwvfky
I0409 12:01:08.891984   13760 out.go:235]     â–ª Configuring RBAC rules ...
I0409 12:01:08.894980   13760 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0409 12:01:08.894980   13760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0409 12:01:08.894980   13760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0409 12:01:08.894980   13760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0409 12:01:08.894980   13760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0409 12:01:08.894980   13760 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0409 12:01:08.894980   13760 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0409 12:01:08.895979   13760 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0409 12:01:08.895979   13760 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0409 12:01:08.895979   13760 kubeadm.go:310] 
I0409 12:01:08.895979   13760 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0409 12:01:08.895979   13760 kubeadm.go:310] 
I0409 12:01:08.895979   13760 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0409 12:01:08.895979   13760 kubeadm.go:310] 
I0409 12:01:08.895979   13760 kubeadm.go:310]   mkdir -p $HOME/.kube
I0409 12:01:08.895979   13760 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0409 12:01:08.895979   13760 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0409 12:01:08.895979   13760 kubeadm.go:310] 
I0409 12:01:08.895979   13760 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0409 12:01:08.895979   13760 kubeadm.go:310] 
I0409 12:01:08.895979   13760 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0409 12:01:08.895979   13760 kubeadm.go:310] 
I0409 12:01:08.895979   13760 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0409 12:01:08.895979   13760 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0409 12:01:08.895979   13760 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0409 12:01:08.895979   13760 kubeadm.go:310] 
I0409 12:01:08.896980   13760 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0409 12:01:08.896980   13760 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0409 12:01:08.896980   13760 kubeadm.go:310] 
I0409 12:01:08.896980   13760 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token qhcmb5.xrpe2hv5fpiwvfky \
I0409 12:01:08.896980   13760 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c564f415fa8f59673cca0a7418284b3339bc1e5bb78075ccea217108a95715c1 \
I0409 12:01:08.896980   13760 kubeadm.go:310] 	--control-plane 
I0409 12:01:08.896980   13760 kubeadm.go:310] 
I0409 12:01:08.896980   13760 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0409 12:01:08.896980   13760 kubeadm.go:310] 
I0409 12:01:08.896980   13760 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token qhcmb5.xrpe2hv5fpiwvfky \
I0409 12:01:08.903987   13760 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:c564f415fa8f59673cca0a7418284b3339bc1e5bb78075ccea217108a95715c1 
I0409 12:01:08.903987   13760 cni.go:84] Creating CNI manager for ""
I0409 12:01:08.903987   13760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0409 12:01:08.908920   13760 out.go:177] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I0409 12:01:08.933086   13760 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0409 12:01:09.018503   13760 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0409 12:01:09.100268   13760 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0409 12:01:09.138062   13760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_04_09T12_01_09_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0409 12:01:09.142097   13760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0409 12:01:09.196268   13760 ops.go:34] apiserver oom_adj: -16
I0409 12:01:10.022630   13760 kubeadm.go:1113] duration metric: took 922.3618ms to wait for elevateKubeSystemPrivileges
I0409 12:01:10.023159   13760 kubeadm.go:394] duration metric: took 21.0198508s to StartCluster
I0409 12:01:10.023488   13760 settings.go:142] acquiring lock: {Name:mk1dc43883b192668fc34902159f52a719564aae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:01:10.024002   13760 settings.go:150] Updating kubeconfig:  C:\Users\Administrator\.kube\config
I0409 12:01:10.025930   13760 lock.go:35] WriteFile acquiring C:\Users\Administrator\.kube\config: {Name:mkea657c15dc45de69286aed2fa89967aee013e4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0409 12:01:10.029406   13760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0409 12:01:10.030655   13760 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0409 12:01:10.033120   13760 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0409 12:01:10.033120   13760 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0409 12:01:10.031507   13760 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0409 12:01:10.034542   13760 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0409 12:01:10.034542   13760 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0409 12:01:10.035712   13760 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0409 12:01:10.036273   13760 host.go:66] Checking if "minikube" exists ...
I0409 12:01:10.044399   13760 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0409 12:01:10.117065   13760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0409 12:01:10.129503   13760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0409 12:01:10.137804   13760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0409 12:01:10.408588   13760 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0409 12:01:10.411330   13760 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0409 12:01:10.411330   13760 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0409 12:01:10.422416   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 12:01:10.510194   13760 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0409 12:01:10.510935   13760 host.go:66] Checking if "minikube" exists ...
I0409 12:01:10.532983   13760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0409 12:01:10.631818   13760 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0409 12:01:10.654239   13760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:6772 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0409 12:01:10.661254   13760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0409 12:01:10.720427   13760 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0409 12:01:10.720427   13760 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0409 12:01:10.732748   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0409 12:01:10.952930   13760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:6772 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0409 12:01:11.245635   13760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0409 12:01:11.415863   13760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0409 12:01:11.671453   13760 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.0396347s)
I0409 12:01:11.671453   13760 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.0101987s)
I0409 12:01:11.674207   13760 start.go:971] {"host.minikube.internal": 192.168.65.2} host record injected into CoreDNS's ConfigMap
I0409 12:01:11.686660   13760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0409 12:01:11.881657   13760 api_server.go:52] waiting for apiserver process to appear ...
I0409 12:01:11.901669   13760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0409 12:01:12.186401   13760 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0409 12:01:12.235078   13760 api_server.go:72] duration metric: took 2.204423s to wait for apiserver process to appear ...
I0409 12:01:12.235078   13760 api_server.go:88] waiting for apiserver healthz status ...
I0409 12:01:12.235078   13760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:6771/healthz ...
I0409 12:01:12.257216   13760 api_server.go:279] https://127.0.0.1:6771/healthz returned 200:
ok
I0409 12:01:12.263879   13760 api_server.go:141] control plane version: v1.32.0
I0409 12:01:12.263917   13760 api_server.go:131] duration metric: took 28.8396ms to wait for apiserver health ...
I0409 12:01:12.263956   13760 system_pods.go:43] waiting for kube-system pods to appear ...
I0409 12:01:12.286994   13760 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I0409 12:01:12.290890   13760 addons.go:514] duration metric: took 2.2615566s for enable addons: enabled=[storage-provisioner default-storageclass]
I0409 12:01:12.301891   13760 system_pods.go:59] 5 kube-system pods found
I0409 12:01:12.302892   13760 system_pods.go:61] "etcd-minikube" [f493fc5d-7f12-4d59-bed2-51a44bce2eb0] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0409 12:01:12.302892   13760 system_pods.go:61] "kube-apiserver-minikube" [d7c6e60c-e010-4b3f-9c3b-41e264ea28d4] Running
I0409 12:01:12.302892   13760 system_pods.go:61] "kube-controller-manager-minikube" [aaea2980-25b8-4b36-9d8d-e15138592677] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0409 12:01:12.302892   13760 system_pods.go:61] "kube-scheduler-minikube" [8cb15c97-2434-4265-a4fa-ab77cb8fd8a2] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0409 12:01:12.302892   13760 system_pods.go:61] "storage-provisioner" [fe2befd6-4bf0-4c1c-b897-61cb5426b1e1] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0409 12:01:12.302892   13760 system_pods.go:74] duration metric: took 38.9364ms to wait for pod list to return data ...
I0409 12:01:12.302892   13760 kubeadm.go:582] duration metric: took 2.2722374s to wait for: map[apiserver:true system_pods:true]
I0409 12:01:12.302892   13760 node_conditions.go:102] verifying NodePressure condition ...
I0409 12:01:12.324672   13760 node_conditions.go:122] node storage ephemeral capacity is 65739308Ki
I0409 12:01:12.324672   13760 node_conditions.go:123] node cpu capacity is 2
I0409 12:01:12.325552   13760 node_conditions.go:105] duration metric: took 22.6596ms to run NodePressure ...
I0409 12:01:12.325552   13760 start.go:241] waiting for startup goroutines ...
I0409 12:01:12.325552   13760 start.go:246] waiting for cluster config update ...
I0409 12:01:12.325552   13760 start.go:255] writing updated cluster config ...
I0409 12:01:12.380422   13760 ssh_runner.go:195] Run: rm -f paused
I0409 12:01:12.647004   13760 start.go:600] kubectl: 1.25.4, cluster: 1.32.0 (minor skew: 7)
I0409 12:01:12.648200   13760 out.go:201] 
W0409 12:01:12.650834   13760 out.go:270] â—  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.25.4, which may have incompatibilities with Kubernetes 1.32.0.
I0409 12:01:12.653053   13760 out.go:177]     â–ª Want kubectl v1.32.0? Try 'minikube kubectl -- get pods -A'
I0409 12:01:12.656092   13760 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 09 06:30:04 minikube dockerd[932]: time="2025-04-09T06:30:04.018048200Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Apr 09 06:30:04 minikube systemd[1]: docker.service: Deactivated successfully.
Apr 09 06:30:04 minikube systemd[1]: Stopped Docker Application Container Engine.
Apr 09 06:30:04 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.098733000Z" level=info msg="Starting up"
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.101053800Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.149528600Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.170610400Z" level=info msg="Loading containers: start."
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.452663200Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.573903600Z" level=info msg="Loading containers: done."
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.593876600Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.594050300Z" level=info msg="Daemon has completed initialization"
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.661598300Z" level=info msg="API listen on [::]:2376"
Apr 09 06:30:04 minikube systemd[1]: Started Docker Application Container Engine.
Apr 09 06:30:04 minikube dockerd[1201]: time="2025-04-09T06:30:04.665405200Z" level=info msg="API listen on /var/run/docker.sock"
Apr 09 06:30:05 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Start docker client with request timeout 0s"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Hairpin mode is set to hairpin-veth"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Loaded network plugin cni"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Docker cri networking managed by network plugin cni"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Setting cgroupDriver cgroupfs"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 09 06:30:06 minikube cri-dockerd[1475]: time="2025-04-09T06:30:06Z" level=info msg="Start cri-dockerd grpc backend"
Apr 09 06:30:06 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 09 06:30:43 minikube systemd[1]: Stopping Docker Application Container Engine...
Apr 09 06:30:43 minikube dockerd[1201]: time="2025-04-09T06:30:43.294973100Z" level=info msg="Processing signal 'terminated'"
Apr 09 06:30:43 minikube dockerd[1201]: time="2025-04-09T06:30:43.309950700Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Apr 09 06:30:43 minikube dockerd[1201]: time="2025-04-09T06:30:43.310969200Z" level=info msg="Daemon shutdown complete"
Apr 09 06:30:43 minikube systemd[1]: docker.service: Deactivated successfully.
Apr 09 06:30:43 minikube systemd[1]: Stopped Docker Application Container Engine.
Apr 09 06:30:43 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 09 06:30:43 minikube dockerd[1647]: time="2025-04-09T06:30:43.516299100Z" level=info msg="Starting up"
Apr 09 06:30:43 minikube dockerd[1647]: time="2025-04-09T06:30:43.519842800Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Apr 09 06:30:43 minikube dockerd[1647]: time="2025-04-09T06:30:43.597125900Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Apr 09 06:30:44 minikube dockerd[1647]: time="2025-04-09T06:30:44.226519700Z" level=info msg="Loading containers: start."
Apr 09 06:30:44 minikube dockerd[1647]: time="2025-04-09T06:30:44.569303900Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 09 06:30:44 minikube dockerd[1647]: time="2025-04-09T06:30:44.732351000Z" level=info msg="Loading containers: done."
Apr 09 06:30:44 minikube dockerd[1647]: time="2025-04-09T06:30:44.756252800Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Apr 09 06:30:44 minikube dockerd[1647]: time="2025-04-09T06:30:44.756428000Z" level=info msg="Daemon has completed initialization"
Apr 09 06:30:44 minikube systemd[1]: Started Docker Application Container Engine.
Apr 09 06:30:44 minikube dockerd[1647]: time="2025-04-09T06:30:44.857665600Z" level=info msg="API listen on /var/run/docker.sock"
Apr 09 06:30:44 minikube dockerd[1647]: time="2025-04-09T06:30:44.857767900Z" level=info msg="API listen on [::]:2376"
Apr 09 06:30:58 minikube cri-dockerd[1475]: time="2025-04-09T06:30:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1e058576567a96d5f201f6022d60f6408d7fd08ec2fe1ee54317ee2b773dea7b/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Apr 09 06:30:58 minikube cri-dockerd[1475]: time="2025-04-09T06:30:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4a1ce69779dcc5391d9f40e46f13c6cbac82931fda11927c08c3b108e5f18cba/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Apr 09 06:30:58 minikube cri-dockerd[1475]: time="2025-04-09T06:30:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0b8179ca3191051d0aeaff95493972d0d655a219154fd0564e98c53f1ed4c0e5/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Apr 09 06:30:58 minikube cri-dockerd[1475]: time="2025-04-09T06:30:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/59ebc3d9a76ad7045c4f45aeb57ffadd952a20cd3962f0690965b3f28a6786fb/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Apr 09 06:31:14 minikube cri-dockerd[1475]: time="2025-04-09T06:31:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c3e0ae291093f7767190cb7c9fc5ab2825d642282ba53a81dc0a536da66d4564/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Apr 09 06:31:14 minikube cri-dockerd[1475]: time="2025-04-09T06:31:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5232156d4be8a540bd8fd3fc6e4c6f4b67fb684be8202603d41041061ce5d55c/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Apr 09 06:31:15 minikube cri-dockerd[1475]: time="2025-04-09T06:31:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8486247aea9eb5bf9afa264704b766798d11ee11d38e11894d1a727932b7e83b/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Apr 09 06:31:19 minikube cri-dockerd[1475]: time="2025-04-09T06:31:19Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Apr 09 06:31:47 minikube cri-dockerd[1475]: time="2025-04-09T06:31:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/61e2f878d555b5ad69e0bb1c386c806d62cb0397d7d5d7468ef4ebb470f78774/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 09 06:31:51 minikube dockerd[1647]: time="2025-04-09T06:31:51.321685400Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 09 06:31:51 minikube dockerd[1647]: time="2025-04-09T06:31:51.323463900Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 09 06:32:06 minikube dockerd[1647]: time="2025-04-09T06:32:06.354691200Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 09 06:32:06 minikube dockerd[1647]: time="2025-04-09T06:32:06.355845000Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 09 06:32:35 minikube dockerd[1647]: time="2025-04-09T06:32:35.859313400Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 09 06:32:35 minikube dockerd[1647]: time="2025-04-09T06:32:35.859463000Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
2ef2e6f984cdb       c69fa2e9cbf5f       About a minute ago   Running             coredns                   0                   8486247aea9eb       coredns-668d6bf9bc-fhl6c
033bcee3097f0       6e38f40d628db       About a minute ago   Running             storage-provisioner       0                   5232156d4be8a       storage-provisioner
47d33f2f5b1bc       040f9f8aac8cd       About a minute ago   Running             kube-proxy                0                   c3e0ae291093f       kube-proxy-snn7c
e4c8bfb243316       a389e107f4ff1       About a minute ago   Running             kube-scheduler            0                   59ebc3d9a76ad       kube-scheduler-minikube
dbf11d3d3e603       a9e7e6b294baf       About a minute ago   Running             etcd                      0                   0b8179ca31910       etcd-minikube
ed9fac570f046       8cab3d2a8bd0f       About a minute ago   Running             kube-controller-manager   0                   4a1ce69779dcc       kube-controller-manager-minikube
e81ea1eb90392       c2e17b8d0f4a3       About a minute ago   Running             kube-apiserver            0                   1e058576567a9       kube-apiserver-minikube


==> coredns [2ef2e6f984cd] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = d488ccc1dc3aabb357df7a36e790c5703f69a16ea73a855fe51f6a72b3b5ba4bfbcf6cb8bdcdce2eb30a503391497295294fdbddea13383093ab27139c9de11f
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:53777 - 24144 "HINFO IN 2885396681148175688.8186250045054835988. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.0267534s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_09T12_01_09_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 09 Apr 2025 06:31:04 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 09 Apr 2025 06:32:51 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 09 Apr 2025 06:31:19 +0000   Wed, 09 Apr 2025 06:31:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 09 Apr 2025 06:31:19 +0000   Wed, 09 Apr 2025 06:31:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 09 Apr 2025 06:31:19 +0000   Wed, 09 Apr 2025 06:31:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 09 Apr 2025 06:31:19 +0000   Wed, 09 Apr 2025 06:31:04 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  65739308Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2032564Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  65739308Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             2032564Ki
  pods:               110
System Info:
  Machine ID:                 aabdc4cc9ce343cfb6d9524d1fd854eb
  System UUID:                ea935009-9b5f-48ed-a60e-a2b270142d7a
  Boot ID:                    80b6ffc3-1a81-4e7e-a979-008f6d0cfcb5
  Kernel Version:             5.15.49-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     book-scraper-d4d5dd849-pvphs        0 (0%)        0 (0%)      0 (0%)           0 (0%)         66s
  kube-system                 coredns-668d6bf9bc-fhl6c            100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     99s
  kube-system                 etcd-minikube                       100m (5%)     0 (0%)      100Mi (5%)       0 (0%)         107s
  kube-system                 kube-apiserver-minikube             250m (12%)    0 (0%)      0 (0%)           0 (0%)         103s
  kube-system                 kube-controller-manager-minikube    200m (10%)    0 (0%)      0 (0%)           0 (0%)         106s
  kube-system                 kube-proxy-snn7c                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         99s
  kube-system                 kube-scheduler-minikube             100m (5%)     0 (0%)      0 (0%)           0 (0%)         107s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         100s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (8%)  170Mi (8%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                  From             Message
  ----     ------                             ----                 ----             -------
  Normal   Starting                           96s                  kube-proxy       
  Normal   NodeHasSufficientMemory            115s (x8 over 115s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              115s (x8 over 115s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               115s (x7 over 115s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            115s                 kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  104s                 kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           104s                 kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced            104s                 kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            103s                 kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              103s                 kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               103s                 kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     100s                 node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Apr 9 05:05] PCI: System does not support PCI
[  +0.094586] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.022934] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.016698] fail to initialize ptp_kvm
[  +0.000001] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.117735] FAT-fs (sr0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.004258] FAT-fs (sr0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +8.077414] grpcfuse: loading out-of-tree module taints kernel.
[Apr 9 06:16] hrtimer: interrupt took 2973900 ns
[Apr 9 06:30] tmpfs: Unknown parameter 'noswap'
[Apr 9 06:31] tmpfs: Unknown parameter 'noswap'


==> etcd [dbf11d3d3e60] <==
{"level":"warn","ts":"2025-04-09T06:31:00.041939Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-09T06:31:00.042716Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-04-09T06:31:00.043266Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-09T06:31:00.043329Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-09T06:31:00.043719Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-09T06:31:00.046465Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-04-09T06:31:00.048928Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-04-09T06:31:00.096648Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"35.1222ms"}
{"level":"info","ts":"2025-04-09T06:31:00.153395Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-04-09T06:31:00.153583Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-04-09T06:31:00.153685Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-04-09T06:31:00.153738Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-04-09T06:31:00.153762Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-04-09T06:31:00.153841Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-04-09T06:31:00.207499Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-04-09T06:31:00.218008Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-04-09T06:31:00.225316Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-04-09T06:31:00.236058Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-04-09T06:31:00.236945Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-09T06:31:00.246234Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-04-09T06:31:00.246647Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-09T06:31:00.246824Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-09T06:31:00.247008Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-09T06:31:00.272512Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-09T06:31:00.272729Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-09T06:31:00.319079Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-09T06:31:00.319500Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-09T06:31:00.319653Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-09T06:31:00.320060Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-09T06:31:00.320250Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-09T06:31:01.054247Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-04-09T06:31:01.055078Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-04-09T06:31:01.055623Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-04-09T06:31:01.055881Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-04-09T06:31:01.056028Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-09T06:31:01.056132Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-04-09T06:31:01.056282Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-09T06:31:01.081349Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-09T06:31:01.092491Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-09T06:31:01.093254Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-09T06:31:01.096705Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-09T06:31:01.097251Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-09T06:31:01.097687Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-09T06:31:01.097794Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-09T06:31:01.098554Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-09T06:31:01.101730Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-04-09T06:31:01.104805Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-09T06:31:01.108593Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-09T06:31:01.109454Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-09T06:31:01.140652Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-09T06:31:10.280992Z","caller":"traceutil/trace.go:171","msg":"trace[209330661] transaction","detail":"{read_only:false; number_of_response:0; response_revision:306; }","duration":"108.0592ms","start":"2025-04-09T06:31:10.172916Z","end":"2025-04-09T06:31:10.280975Z","steps":["trace[209330661] 'process raft request'  (duration: 68.4418ms)","trace[209330661] 'compare'  (duration: 39.4964ms)"],"step_count":2}
{"level":"info","ts":"2025-04-09T06:31:10.284782Z","caller":"traceutil/trace.go:171","msg":"trace[204321332] transaction","detail":"{read_only:false; number_of_response:0; response_revision:306; }","duration":"108.2548ms","start":"2025-04-09T06:31:10.176510Z","end":"2025-04-09T06:31:10.284764Z","steps":["trace[204321332] 'process raft request'  (duration: 104.4246ms)"],"step_count":1}
{"level":"info","ts":"2025-04-09T06:31:10.284817Z","caller":"traceutil/trace.go:171","msg":"trace[933098799] transaction","detail":"{read_only:false; number_of_response:0; response_revision:306; }","duration":"108.4018ms","start":"2025-04-09T06:31:10.176412Z","end":"2025-04-09T06:31:10.284814Z","steps":["trace[933098799] 'process raft request'  (duration: 104.4976ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-09T06:31:10.300843Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.4431ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-controller-manager-minikube\" limit:1 ","response":"range_response_count:1 size:7253"}
{"level":"info","ts":"2025-04-09T06:31:10.312278Z","caller":"traceutil/trace.go:171","msg":"trace[1963036607] range","detail":"{range_begin:/registry/pods/kube-system/kube-controller-manager-minikube; range_end:; response_count:1; response_revision:307; }","duration":"121.8931ms","start":"2025-04-09T06:31:10.190362Z","end":"2025-04-09T06:31:10.312255Z","steps":["trace[1963036607] 'agreement among raft nodes before linearized reading'  (duration: 110.4286ms)"],"step_count":1}


==> kernel <==
 06:32:52 up  1:27,  0 users,  load average: 0.97, 1.59, 0.93
Linux minikube 5.15.49-linuxkit #1 SMP Tue Sep 13 07:51:46 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [e81ea1eb9039] <==
I0409 06:31:03.923470       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0409 06:31:03.925665       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0409 06:31:03.926166       1 aggregator.go:169] waiting for initial CRD sync...
I0409 06:31:03.926613       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0409 06:31:03.927099       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0409 06:31:03.927220       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0409 06:31:03.928400       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0409 06:31:03.928678       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0409 06:31:03.929528       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0409 06:31:03.929924       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0409 06:31:03.929972       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0409 06:31:03.930596       1 controller.go:142] Starting OpenAPI controller
I0409 06:31:03.930757       1 controller.go:90] Starting OpenAPI V3 controller
I0409 06:31:03.930858       1 naming_controller.go:294] Starting NamingConditionController
I0409 06:31:03.930908       1 establishing_controller.go:81] Starting EstablishingController
I0409 06:31:03.930935       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0409 06:31:03.930990       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0409 06:31:03.931123       1 crd_finalizer.go:269] Starting CRDFinalizer
I0409 06:31:03.933405       1 controller.go:119] Starting legacy_token_tracking_controller
I0409 06:31:03.933537       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0409 06:31:03.947219       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0409 06:31:03.947733       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0409 06:31:03.947770       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0409 06:31:03.948136       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0409 06:31:03.948164       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0409 06:31:03.948220       1 controller.go:78] Starting OpenAPI AggregationController
I0409 06:31:04.146085       1 shared_informer.go:320] Caches are synced for node_authorizer
I0409 06:31:04.162743       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0409 06:31:04.164120       1 policy_source.go:240] refreshing policies
I0409 06:31:04.226795       1 cache.go:39] Caches are synced for LocalAvailability controller
I0409 06:31:04.227048       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0409 06:31:04.227761       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0409 06:31:04.228110       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0409 06:31:04.230051       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0409 06:31:04.230708       1 aggregator.go:171] initial CRD sync complete...
I0409 06:31:04.230738       1 autoregister_controller.go:144] Starting autoregister controller
I0409 06:31:04.230745       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0409 06:31:04.230750       1 cache.go:39] Caches are synced for autoregister controller
I0409 06:31:04.238154       1 controller.go:615] quota admission added evaluator for: namespaces
I0409 06:31:04.238242       1 shared_informer.go:320] Caches are synced for configmaps
I0409 06:31:04.249759       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0409 06:31:04.250073       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0409 06:31:04.250226       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0409 06:31:04.354634       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0409 06:31:04.956487       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0409 06:31:04.972632       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0409 06:31:04.972771       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0409 06:31:06.923454       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0409 06:31:07.108443       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0409 06:31:07.376666       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0409 06:31:07.505664       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0409 06:31:07.507455       1 controller.go:615] quota admission added evaluator for: endpoints
I0409 06:31:07.528480       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0409 06:31:08.160378       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0409 06:31:08.382051       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0409 06:31:08.473466       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0409 06:31:08.554863       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0409 06:31:13.441389       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0409 06:31:13.640182       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0409 06:32:03.695427       1 alloc.go:330] "allocated clusterIPs" service="default/book-scraper-service" clusterIPs={"IPv4":"10.111.29.214"}


==> kube-controller-manager [ed9fac570f04] <==
I0409 06:31:12.696638       1 shared_informer.go:320] Caches are synced for GC
I0409 06:31:12.696950       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0409 06:31:12.697344       1 shared_informer.go:320] Caches are synced for HPA
I0409 06:31:12.699252       1 shared_informer.go:320] Caches are synced for ReplicationController
I0409 06:31:12.715355       1 shared_informer.go:320] Caches are synced for crt configmap
I0409 06:31:12.721892       1 shared_informer.go:320] Caches are synced for job
I0409 06:31:12.732755       1 shared_informer.go:320] Caches are synced for TTL after finished
I0409 06:31:12.748949       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0409 06:31:12.754609       1 shared_informer.go:320] Caches are synced for TTL
I0409 06:31:12.755895       1 shared_informer.go:320] Caches are synced for namespace
I0409 06:31:12.881006       1 shared_informer.go:320] Caches are synced for expand
I0409 06:31:12.883500       1 shared_informer.go:320] Caches are synced for ephemeral
I0409 06:31:12.884885       1 shared_informer.go:320] Caches are synced for PVC protection
I0409 06:31:12.892264       1 shared_informer.go:320] Caches are synced for stateful set
I0409 06:31:12.906484       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0409 06:31:12.906937       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0409 06:31:12.908074       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0409 06:31:12.913248       1 shared_informer.go:320] Caches are synced for daemon sets
I0409 06:31:12.914456       1 shared_informer.go:320] Caches are synced for attach detach
I0409 06:31:12.936426       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0409 06:31:12.936528       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0409 06:31:12.936600       1 shared_informer.go:320] Caches are synced for disruption
I0409 06:31:12.937870       1 shared_informer.go:320] Caches are synced for PV protection
I0409 06:31:12.939102       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0409 06:31:12.939208       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0409 06:31:12.939262       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0409 06:31:12.941355       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0409 06:31:12.941838       1 shared_informer.go:320] Caches are synced for resource quota
I0409 06:31:12.947062       1 shared_informer.go:320] Caches are synced for resource quota
I0409 06:31:12.949275       1 shared_informer.go:320] Caches are synced for garbage collector
I0409 06:31:12.951622       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0409 06:31:12.951677       1 shared_informer.go:320] Caches are synced for endpoint
I0409 06:31:12.959245       1 shared_informer.go:320] Caches are synced for deployment
I0409 06:31:12.962315       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0409 06:31:12.963306       1 shared_informer.go:320] Caches are synced for cronjob
I0409 06:31:12.969421       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0409 06:31:12.972284       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0409 06:31:13.000921       1 shared_informer.go:320] Caches are synced for persistent volume
I0409 06:31:13.004328       1 shared_informer.go:320] Caches are synced for garbage collector
I0409 06:31:13.004544       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0409 06:31:13.004632       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0409 06:31:13.085922       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0409 06:31:13.103360       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0409 06:31:13.905641       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="218.1334ms"
I0409 06:31:13.937593       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="31.8522ms"
I0409 06:31:13.938051       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="327.3Âµs"
I0409 06:31:13.943568       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="227Âµs"
I0409 06:31:16.291031       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="1.1279ms"
I0409 06:31:19.157650       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0409 06:31:28.341644       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="17.5724ms"
I0409 06:31:28.342193       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="124.8Âµs"
I0409 06:31:46.368658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="156.5918ms"
I0409 06:31:46.389109       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="19.2115ms"
I0409 06:31:46.389394       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="55.3Âµs"
I0409 06:31:46.410864       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="56.4Âµs"
I0409 06:31:51.736939       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="78.5Âµs"
I0409 06:32:02.729518       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="85.8Âµs"
I0409 06:32:19.711397       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="46.5Âµs"
I0409 06:32:32.763877       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="84.6Âµs"
I0409 06:32:48.722022       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/book-scraper-d4d5dd849" duration="44.4Âµs"


==> kube-proxy [47d33f2f5b1b] <==
I0409 06:31:15.819280       1 server_linux.go:66] "Using iptables proxy"
I0409 06:31:16.015855       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0409 06:31:16.016760       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0409 06:31:16.067610       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0409 06:31:16.067834       1 server_linux.go:170] "Using iptables Proxier"
I0409 06:31:16.072807       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0409 06:31:16.078333       1 server.go:497] "Version info" version="v1.32.0"
I0409 06:31:16.078518       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0409 06:31:16.098319       1 config.go:199] "Starting service config controller"
I0409 06:31:16.098427       1 shared_informer.go:313] Waiting for caches to sync for service config
I0409 06:31:16.098466       1 config.go:105] "Starting endpoint slice config controller"
I0409 06:31:16.098575       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0409 06:31:16.098660       1 config.go:329] "Starting node config controller"
I0409 06:31:16.104830       1 shared_informer.go:313] Waiting for caches to sync for node config
I0409 06:31:16.199778       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0409 06:31:16.199864       1 shared_informer.go:320] Caches are synced for service config
I0409 06:31:16.205410       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [e4c8bfb24331] <==
E0409 06:31:04.342005       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338093       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0409 06:31:04.344445       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338145       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:04.344627       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338289       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0409 06:31:04.344785       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338333       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:04.345050       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338381       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0409 06:31:04.345237       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338419       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0409 06:31:04.345395       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338469       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0409 06:31:04.346577       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338510       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0409 06:31:04.347041       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338559       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0409 06:31:04.347227       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338595       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0409 06:31:04.347379       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338612       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0409 06:31:04.347547       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:04.338655       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0409 06:31:04.347688       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0409 06:31:04.338772       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:04.347930       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.183678       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:05.183722       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.245063       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:05.245138       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.272000       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0409 06:31:05.272815       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.388441       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0409 06:31:05.388595       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.412645       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:05.413284       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.416062       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0409 06:31:05.416218       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0409 06:31:05.428364       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0409 06:31:05.428664       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.509445       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:05.509947       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.683795       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0409 06:31:05.683917       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.756941       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0409 06:31:05.756996       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.766491       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0409 06:31:05.766809       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.793671       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0409 06:31:05.793773       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.811576       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0409 06:31:05.813324       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.856339       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0409 06:31:05.856528       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.856620       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0409 06:31:05.857024       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0409 06:31:05.925460       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0409 06:31:05.925551       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
I0409 06:31:07.482539       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.048121    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.048348    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-certs\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.048381    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-data\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.048417    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.048491    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: E0409 06:31:09.077324    2572 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: E0409 06:31:09.087519    2572 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: E0409 06:31:09.088680    2572 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.095733    2572 kubelet_node_status.go:125] "Node was previously registered" node="minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.095997    2572 kubelet_node_status.go:79] "Successfully registered node" node="minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.113381    2572 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.151638    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.155390    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.161278    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.161554    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.162333    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.165112    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.165440    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/d14ce008bee3a1f3bd7cf547688f9dfe-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"d14ce008bee3a1f3bd7cf547688f9dfe\") " pod="kube-system/kube-scheduler-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.165976    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.543316    2572 apiserver.go:52] "Watching apiserver"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.618511    2572 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Apr 09 06:31:09 minikube kubelet[2572]: I0409 06:31:09.668659    2572 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=0.6686364 podStartE2EDuration="668.6364ms" podCreationTimestamp="2025-04-09 06:31:09 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-09 06:31:09.6399421 +0000 UTC m=+1.383908601" watchObservedRunningTime="2025-04-09 06:31:09.6686364 +0000 UTC m=+1.412602801"
Apr 09 06:31:10 minikube kubelet[2572]: I0409 06:31:10.107139    2572 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:10 minikube kubelet[2572]: I0409 06:31:10.119743    2572 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Apr 09 06:31:10 minikube kubelet[2572]: I0409 06:31:10.121016    2572 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Apr 09 06:31:10 minikube kubelet[2572]: I0409 06:31:10.125741    2572 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Apr 09 06:31:10 minikube kubelet[2572]: E0409 06:31:10.314983    2572 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Apr 09 06:31:10 minikube kubelet[2572]: E0409 06:31:10.322080    2572 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Apr 09 06:31:10 minikube kubelet[2572]: E0409 06:31:10.322488    2572 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Apr 09 06:31:10 minikube kubelet[2572]: E0409 06:31:10.322730    2572 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Apr 09 06:31:13 minikube kubelet[2572]: I0409 06:31:13.625237    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bm9vt\" (UniqueName: \"kubernetes.io/projected/10ec8c4d-7bd1-40f5-b08d-d6292392bb9c-kube-api-access-bm9vt\") pod \"kube-proxy-snn7c\" (UID: \"10ec8c4d-7bd1-40f5-b08d-d6292392bb9c\") " pod="kube-system/kube-proxy-snn7c"
Apr 09 06:31:13 minikube kubelet[2572]: I0409 06:31:13.625306    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/10ec8c4d-7bd1-40f5-b08d-d6292392bb9c-kube-proxy\") pod \"kube-proxy-snn7c\" (UID: \"10ec8c4d-7bd1-40f5-b08d-d6292392bb9c\") " pod="kube-system/kube-proxy-snn7c"
Apr 09 06:31:13 minikube kubelet[2572]: I0409 06:31:13.625330    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/10ec8c4d-7bd1-40f5-b08d-d6292392bb9c-xtables-lock\") pod \"kube-proxy-snn7c\" (UID: \"10ec8c4d-7bd1-40f5-b08d-d6292392bb9c\") " pod="kube-system/kube-proxy-snn7c"
Apr 09 06:31:13 minikube kubelet[2572]: I0409 06:31:13.625351    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/10ec8c4d-7bd1-40f5-b08d-d6292392bb9c-lib-modules\") pod \"kube-proxy-snn7c\" (UID: \"10ec8c4d-7bd1-40f5-b08d-d6292392bb9c\") " pod="kube-system/kube-proxy-snn7c"
Apr 09 06:31:13 minikube kubelet[2572]: I0409 06:31:13.926832    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/fe2befd6-4bf0-4c1c-b897-61cb5426b1e1-tmp\") pod \"storage-provisioner\" (UID: \"fe2befd6-4bf0-4c1c-b897-61cb5426b1e1\") " pod="kube-system/storage-provisioner"
Apr 09 06:31:13 minikube kubelet[2572]: I0409 06:31:13.926890    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w5lzx\" (UniqueName: \"kubernetes.io/projected/fe2befd6-4bf0-4c1c-b897-61cb5426b1e1-kube-api-access-w5lzx\") pod \"storage-provisioner\" (UID: \"fe2befd6-4bf0-4c1c-b897-61cb5426b1e1\") " pod="kube-system/storage-provisioner"
Apr 09 06:31:14 minikube kubelet[2572]: I0409 06:31:14.028272    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/e4e714cd-1cdb-4022-9e6a-57b9aa91792c-config-volume\") pod \"coredns-668d6bf9bc-fhl6c\" (UID: \"e4e714cd-1cdb-4022-9e6a-57b9aa91792c\") " pod="kube-system/coredns-668d6bf9bc-fhl6c"
Apr 09 06:31:14 minikube kubelet[2572]: I0409 06:31:14.028347    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vhqww\" (UniqueName: \"kubernetes.io/projected/e4e714cd-1cdb-4022-9e6a-57b9aa91792c-kube-api-access-vhqww\") pod \"coredns-668d6bf9bc-fhl6c\" (UID: \"e4e714cd-1cdb-4022-9e6a-57b9aa91792c\") " pod="kube-system/coredns-668d6bf9bc-fhl6c"
Apr 09 06:31:14 minikube kubelet[2572]: I0409 06:31:14.185072    2572 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c3e0ae291093f7767190cb7c9fc5ab2825d642282ba53a81dc0a536da66d4564"
Apr 09 06:31:15 minikube kubelet[2572]: I0409 06:31:15.256032    2572 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-snn7c" podStartSLOduration=2.2560073 podStartE2EDuration="2.2560073s" podCreationTimestamp="2025-04-09 06:31:13 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-09 06:31:15.2269988 +0000 UTC m=+6.970965201" watchObservedRunningTime="2025-04-09 06:31:15.2560073 +0000 UTC m=+6.999973701"
Apr 09 06:31:15 minikube kubelet[2572]: I0409 06:31:15.257123    2572 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=3.2570855 podStartE2EDuration="3.2570855s" podCreationTimestamp="2025-04-09 06:31:12 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-09 06:31:15.2551568 +0000 UTC m=+6.999123201" watchObservedRunningTime="2025-04-09 06:31:15.2570855 +0000 UTC m=+7.001051901"
Apr 09 06:31:17 minikube kubelet[2572]: I0409 06:31:17.830356    2572 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-fhl6c" podStartSLOduration=4.8303334 podStartE2EDuration="4.8303334s" podCreationTimestamp="2025-04-09 06:31:13 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-09 06:31:16.2894843 +0000 UTC m=+8.033450701" watchObservedRunningTime="2025-04-09 06:31:17.8303334 +0000 UTC m=+9.574642801"
Apr 09 06:31:19 minikube kubelet[2572]: I0409 06:31:19.134569    2572 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Apr 09 06:31:19 minikube kubelet[2572]: I0409 06:31:19.137207    2572 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Apr 09 06:31:46 minikube kubelet[2572]: I0409 06:31:46.425635    2572 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-n665z\" (UniqueName: \"kubernetes.io/projected/0fdc6490-11ed-41a7-bc10-7215ad35aa29-kube-api-access-n665z\") pod \"book-scraper-d4d5dd849-pvphs\" (UID: \"0fdc6490-11ed-41a7-bc10-7215ad35aa29\") " pod="default/book-scraper-d4d5dd849-pvphs"
Apr 09 06:31:51 minikube kubelet[2572]: E0409 06:31:51.336620    2572 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="book-scraper:latest"
Apr 09 06:31:51 minikube kubelet[2572]: E0409 06:31:51.337529    2572 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="book-scraper:latest"
Apr 09 06:31:51 minikube kubelet[2572]: E0409 06:31:51.339296    2572 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:book-scraper,Image:book-scraper:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n665z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod book-scraper-d4d5dd849-pvphs_default(0fdc6490-11ed-41a7-bc10-7215ad35aa29): ErrImagePull: Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 09 06:31:51 minikube kubelet[2572]: E0409 06:31:51.340553    2572 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"book-scraper\" with ErrImagePull: \"Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/book-scraper-d4d5dd849-pvphs" podUID="0fdc6490-11ed-41a7-bc10-7215ad35aa29"
Apr 09 06:31:51 minikube kubelet[2572]: E0409 06:31:51.714369    2572 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"book-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"book-scraper:latest\\\": ErrImagePull: Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/book-scraper-d4d5dd849-pvphs" podUID="0fdc6490-11ed-41a7-bc10-7215ad35aa29"
Apr 09 06:32:06 minikube kubelet[2572]: E0409 06:32:06.365592    2572 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="book-scraper:latest"
Apr 09 06:32:06 minikube kubelet[2572]: E0409 06:32:06.365674    2572 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="book-scraper:latest"
Apr 09 06:32:06 minikube kubelet[2572]: E0409 06:32:06.365823    2572 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:book-scraper,Image:book-scraper:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n665z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod book-scraper-d4d5dd849-pvphs_default(0fdc6490-11ed-41a7-bc10-7215ad35aa29): ErrImagePull: Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 09 06:32:06 minikube kubelet[2572]: E0409 06:32:06.367511    2572 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"book-scraper\" with ErrImagePull: \"Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/book-scraper-d4d5dd849-pvphs" podUID="0fdc6490-11ed-41a7-bc10-7215ad35aa29"
Apr 09 06:32:19 minikube kubelet[2572]: E0409 06:32:19.694672    2572 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"book-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"book-scraper:latest\\\": ErrImagePull: Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/book-scraper-d4d5dd849-pvphs" podUID="0fdc6490-11ed-41a7-bc10-7215ad35aa29"
Apr 09 06:32:35 minikube kubelet[2572]: E0409 06:32:35.933250    2572 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="book-scraper:latest"
Apr 09 06:32:35 minikube kubelet[2572]: E0409 06:32:35.933326    2572 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="book-scraper:latest"
Apr 09 06:32:35 minikube kubelet[2572]: E0409 06:32:35.933427    2572 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:book-scraper,Image:book-scraper:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n665z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod book-scraper-d4d5dd849-pvphs_default(0fdc6490-11ed-41a7-bc10-7215ad35aa29): ErrImagePull: Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 09 06:32:35 minikube kubelet[2572]: E0409 06:32:35.936270    2572 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"book-scraper\" with ErrImagePull: \"Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/book-scraper-d4d5dd849-pvphs" podUID="0fdc6490-11ed-41a7-bc10-7215ad35aa29"
Apr 09 06:32:48 minikube kubelet[2572]: E0409 06:32:48.700989    2572 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"book-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"book-scraper:latest\\\": ErrImagePull: Error response from daemon: pull access denied for book-scraper, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/book-scraper-d4d5dd849-pvphs" podUID="0fdc6490-11ed-41a7-bc10-7215ad35aa29"


==> storage-provisioner [033bcee3097f] <==
I0409 06:31:14.925495       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0409 06:31:20.976753       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0409 06:31:20.977034       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0409 06:31:20.992885       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0409 06:31:20.993145       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_80f34005-0b97-4910-8cf3-61735392f06c!
I0409 06:31:20.994370       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"45e568ea-ebc4-491a-bbef-4b707a00457e", APIVersion:"v1", ResourceVersion:"411", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_80f34005-0b97-4910-8cf3-61735392f06c became leader
I0409 06:31:21.094073       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_80f34005-0b97-4910-8cf3-61735392f06c!

